{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be081c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f9bce",
   "metadata": {},
   "source": [
    "# **The Mathematical Derivation for a 3-Layer Network**\n",
    "\n",
    "Let's assume a 3-layer architecture: $X \\longrightarrow H_1 \\longrightarrow H_2 \\longrightarrow \\text{Output}$\n",
    "\n",
    "\n",
    "### **1. Forward Pass Equations**\n",
    "\n",
    "* **Layer 1**: $Z^{(1)} = W^{(1)} A^{(0)} + b^{(1)}$ , where $\\quad A^{(0)} = X$\n",
    "* **Activation 1**: $A^{(1)} = \\sigma\\!\\left(Z^{(1)}\\right)$ (ReLU)\n",
    "* **Layer 2**: $Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}$\n",
    "* **Activation 2**: $A^{(2)} = \\sigma\\!\\left(Z^{(2)}\\right)$ (ReLU)\n",
    "* **Layer 3 (Output)**: $Z^{(3)} = W^{(3)} A^{(2)} + b^{(3)}$\n",
    "* **Final Prediction**: $\\hat{y} = \\operatorname{Softmax}\\!\\left(Z^{(3)}\\right)$\n",
    "\n",
    "### **2. The Loss Function (Cross-Entropy)**\n",
    "\n",
    "$$\\mathcal{L}(y, \\hat{y}) = -\\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "### **3. Backpropagation (Chain Rule)**\n",
    "\n",
    "For the output layer, the derivative of the Loss with respect to the pre-activation  simplifies elegantly to:\n",
    "$$\\delta^{(3)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(3)}} = \\hat{y} - y$$\n",
    "\n",
    "Now, we propagate this error backward to find the gradients for the weights:\n",
    "\n",
    "**For Layer 3:**\n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W^{(3)}} = \\delta^{(3)} \\left(A^{(2)}\\right)^T$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial b^{(3)}} = \\delta^{(3)}$\n",
    "\n",
    "**For Layer 2 (Hidden):**\n",
    "\n",
    "* $\\delta^{(2)} = \\left(W^{(3)}\\right)^T \\delta^{(3)} \\odot \\sigma'\\!\\left(Z^{(2)}\\right)$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\delta^{(2)} \\left(A^{(1)}\\right)^T$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} = \\delta^{(2)}$\n",
    "\n",
    "**For Layer 1 (Hidden):**\n",
    "\n",
    "* $\\delta^{(1)} = \\left(W^{(2)}\\right)^T \\delta^{(2)} \\odot \\sigma'\\!\\left(Z^{(1)}\\right)$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\delta^{(1)} \\left(A^{(0)}\\right)^T$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial b^{(1)}} = \\delta^{(1)}$\n",
    "\n",
    "> **Note**: The symbol $\\odot$ represents the **Hadamard Product** (element-wise multiplication). This is crucial because each neuronâ€™s error is gated by the derivative of its own activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchNet:\n",
    "    def __init__(self, layers=[3072, 256, 128, 10]):\n",
    "        self.params = {}\n",
    "        # He Initialization for all layers\n",
    "        for i in range(1, len(layers)):\n",
    "            self.params[f'W{i}'] = np.random.randn(layers[i], layers[i-1]) * np.sqrt(2./layers[i-1])\n",
    "            self.params[f'b{i}'] = np.zeros((layers[i], 1))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_deriv(self, Z):\n",
    "        return Z > 0\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        # Numeric stability: subtract max(Z)\n",
    "        expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        self.Z1 = np.dot(self.params['W1'], X) + self.params['b1']\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        # Layer 2\n",
    "        self.Z2 = np.dot(self.params['W2'], self.A1) + self.params['b2']\n",
    "        self.A2 = self.relu(self.Z2)\n",
    "        # Layer 3 (Output)\n",
    "        self.Z3 = np.dot(self.params['W3'], self.A2) + self.params['b3']\n",
    "        self.A3 = self.softmax(self.Z3)\n",
    "        return self.A3\n",
    "\n",
    "    def backward(self, X, Y, A3, lr):\n",
    "        m = X.shape[1]\n",
    "        grads = {}\n",
    "        \n",
    "        # Output Error (Softmax + Cross-Entropy derivative)\n",
    "        dZ3 = A3 - Y\n",
    "        grads['dW3'] = (1/m) * np.dot(dZ3, self.A2.T)\n",
    "        grads['db3'] = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "        # Layer 2 Error\n",
    "        dA2 = np.dot(self.params['W3'].T, dZ3)\n",
    "        dZ2 = dA2 * self.relu_deriv(self.Z2)\n",
    "        grads['dW2'] = (1/m) * np.dot(dZ2, self.A1.T)\n",
    "        grads['db2'] = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        # Layer 1 Error\n",
    "        dA1 = np.dot(self.params['W2'].T, dZ2)\n",
    "        dZ1 = dA1 * self.relu_deriv(self.Z1)\n",
    "        grads['dW1'] = (1/m) * np.dot(dZ1, X.T)\n",
    "        grads['db1'] = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        # SGD Update\n",
    "        for i in range(1, 4):\n",
    "            self.params[f'W{i}'] -= lr * grads[f'dW{i}']\n",
    "            self.params[f'b{i}'] -= lr * grads[f'db{i}']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
