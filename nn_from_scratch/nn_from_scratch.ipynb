{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be081c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f9bce",
   "metadata": {},
   "source": [
    "# **The Mathematical Derivation for a 3-Layer Network**\n",
    "\n",
    "Let's assume a 3-layer architecture: $X \\longrightarrow H_1 \\longrightarrow H_2 \\longrightarrow \\text{Output}$\n",
    "\n",
    "\n",
    "### **1. Forward Pass Equations**\n",
    "\n",
    "* **Layer 1**: $Z^{(1)} = W^{(1)} A^{(0)} + b^{(1)}$ , where $\\quad A^{(0)} = X$\n",
    "* **Activation 1**: $A^{(1)} = \\sigma\\!\\left(Z^{(1)}\\right)$ (ReLU)\n",
    "* **Layer 2**: $Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}$\n",
    "* **Activation 2**: $A^{(2)} = \\sigma\\!\\left(Z^{(2)}\\right)$ (ReLU)\n",
    "* **Layer 3 (Output)**: $Z^{(3)} = W^{(3)} A^{(2)} + b^{(3)}$\n",
    "* **Final Prediction**: $\\hat{y} = \\operatorname{Softmax}\\!\\left(Z^{(3)}\\right)$\n",
    "\n",
    "### **2. The Loss Function (Cross-Entropy)**\n",
    "\n",
    "$$\\mathcal{L}(y, \\hat{y}) = -\\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "### **3. Backpropagation (Chain Rule)**\n",
    "\n",
    "For the output layer, the derivative of the Loss with respect to the pre-activation  simplifies elegantly to:\n",
    "$$\\delta^{(3)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(3)}} = \\hat{y} - y$$\n",
    "\n",
    "Now, we propagate this error backward to find the gradients for the weights:\n",
    "\n",
    "**For Layer 3:**\n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W^{(3)}} = \\delta^{(3)} \\left(A^{(2)}\\right)^T$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial b^{(3)}} = \\delta^{(3)}$\n",
    "\n",
    "**For Layer 2 (Hidden):**\n",
    "\n",
    "* $\\delta^{(2)} = \\left(W^{(3)}\\right)^T \\delta^{(3)} \\odot \\sigma'\\!\\left(Z^{(2)}\\right)$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\delta^{(2)} \\left(A^{(1)}\\right)^T$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} = \\delta^{(2)}$\n",
    "\n",
    "**For Layer 1 (Hidden):**\n",
    "\n",
    "* $\\delta^{(1)} = \\left(W^{(2)}\\right)^T \\delta^{(2)} \\odot \\sigma'\\!\\left(Z^{(1)}\\right)$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\delta^{(1)} \\left(A^{(0)}\\right)^T$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial b^{(1)}} = \\delta^{(1)}$\n",
    "\n",
    "> **Note**: The symbol $\\odot$ represents the **Hadamard Product** (element-wise multiplication). This is crucial because each neuronâ€™s error is gated by the derivative of its own activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcacb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchNet:\n",
    "    def __init__(self, layers=[3072, 256, 128, 10]):\n",
    "        self.params = {}\n",
    "        # He Initialization for all layers\n",
    "        for i in range(1, len(layers)):\n",
    "            self.params[f'W{i}'] = np.random.randn(layers[i], layers[i-1]) * np.sqrt(2./layers[i-1])\n",
    "            self.params[f'b{i}'] = np.zeros((layers[i], 1))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_deriv(self, Z):\n",
    "        return Z > 0\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        # Numeric stability: subtract max(Z)\n",
    "        expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        self.Z1 = np.dot(self.params['W1'], X) + self.params['b1']\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        # Layer 2\n",
    "        self.Z2 = np.dot(self.params['W2'], self.A1) + self.params['b2']\n",
    "        self.A2 = self.relu(self.Z2)\n",
    "        # Layer 3 (Output)\n",
    "        self.Z3 = np.dot(self.params['W3'], self.A2) + self.params['b3']\n",
    "        self.A3 = self.softmax(self.Z3)\n",
    "        return self.A3\n",
    "\n",
    "    def backward(self, X, Y, A3, lr):\n",
    "        m = X.shape[1]\n",
    "        grads = {}\n",
    "        \n",
    "        # Output Error (Softmax + Cross-Entropy derivative)\n",
    "        dZ3 = A3 - Y\n",
    "        grads['dW3'] = (1/m) * np.dot(dZ3, self.A2.T)\n",
    "        grads['db3'] = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "        # Layer 2 Error\n",
    "        dA2 = np.dot(self.params['W3'].T, dZ3)\n",
    "        dZ2 = dA2 * self.relu_deriv(self.Z2)\n",
    "        grads['dW2'] = (1/m) * np.dot(dZ2, self.A1.T)\n",
    "        grads['db2'] = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        # Layer 1 Error\n",
    "        dA1 = np.dot(self.params['W2'].T, dZ2)\n",
    "        dZ1 = dA1 * self.relu_deriv(self.Z1)\n",
    "        grads['dW1'] = (1/m) * np.dot(dZ1, X.T)\n",
    "        grads['db1'] = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        # SGD Update\n",
    "        for i in range(1, 4):\n",
    "            self.params[f'W{i}'] -= lr * grads[f'dW{i}']\n",
    "            self.params[f'b{i}'] -= lr * grads[f'db{i}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cf7eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "/home/p_retrover/github/basic_machine_learning/.venv/lib64/python3.14/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "# Download CIFAR-10\n",
    "train_set = torchvision.datasets.CIFAR10(root='./nn_from_scratch/data', train=True, download=True)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./nn_from_scratch/data', train=False, download=True)\n",
    "\n",
    "# Preprocessing: Flatten (32*32*3 = 3072) and Normalize (0-1)\n",
    "# X shape becomes (3072, 50000)\n",
    "X_train = train_set.data.reshape(50000, 3072).T / 255.0\n",
    "X_test = test_set.data.reshape(10000, 3072).T / 255.0\n",
    "\n",
    "# Extract labels as 1D arrays\n",
    "Y_train_labels = np.array(train_set.targets)\n",
    "Y_test_labels = np.array(test_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49184ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y, num_classes=10):\n",
    "    # Y is a 1D array of labels, e.g., [3, 0, 9...]\n",
    "    m = Y.shape[0]\n",
    "    one_hot_Y = np.zeros((num_classes, m))\n",
    "    \n",
    "    # Use advanced indexing: \n",
    "    # For each column 'i', set the row 'Y[i]' to 1\n",
    "    one_hot_Y[Y, np.arange(m)] = 1\n",
    "    \n",
    "    return one_hot_Y\n",
    "# pre-encode training labels\n",
    "Y_train = one_hot(Y_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd8eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size):\n",
    "    # X shape: (3072, 50000), y shape: (10, 50000)\n",
    "    m = X.shape[1]\n",
    "    indices = np.arange(m)\n",
    "    np.random.shuffle(indices) # Shuffle indices for every epoch\n",
    "    \n",
    "    for i in range(0, m, batch_size):\n",
    "        batch_indices = indices[i:i + batch_size]\n",
    "        yield X[:, batch_indices], y[:, batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Loss: 1.9804\n",
      "Epoch 2/20 | Loss: 1.8110\n",
      "Epoch 3/20 | Loss: 1.7476\n",
      "Epoch 4/20 | Loss: 1.7008\n",
      "Epoch 5/20 | Loss: 1.6646\n",
      "Epoch 6/20 | Loss: 1.6305\n",
      "Epoch 7/20 | Loss: 1.6031\n",
      "Epoch 8/20 | Loss: 1.5784\n",
      "Epoch 9/20 | Loss: 1.5600\n",
      "Epoch 10/20 | Loss: 1.5417\n",
      "Epoch 11/20 | Loss: 1.5168\n",
      "Epoch 12/20 | Loss: 1.5028\n",
      "Epoch 13/20 | Loss: 1.4864\n",
      "Epoch 14/20 | Loss: 1.4699\n",
      "Epoch 15/20 | Loss: 1.4566\n",
      "Epoch 16/20 | Loss: 1.4427\n",
      "Epoch 17/20 | Loss: 1.4328\n",
      "Epoch 18/20 | Loss: 1.4150\n",
      "Epoch 19/20 | Loss: 1.4031\n",
      "Epoch 20/20 | Loss: 1.3955\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "model = ScratchNet(layers=[3072, 256, 128, 10])\n",
    "epochs = 20\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for x_batch, y_batch in get_batches(X_train, Y_train, batch_size):\n",
    "        # 1. Forward Pass\n",
    "        A3 = model.forward(x_batch)\n",
    "        \n",
    "        # 2. Compute Loss\n",
    "        loss = -np.mean(np.sum(y_batch * np.log(A3 + 1e-8), axis=0))\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # 3. Backward Pass\n",
    "        model.backward(x_batch, y_batch, A3, lr)\n",
    "        num_batches += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss/num_batches:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
