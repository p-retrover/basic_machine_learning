{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc344b3",
   "metadata": {},
   "source": [
    "# **Logistic Regression Algorithm Logic**\n",
    "\n",
    "## **1. Initialization**\n",
    "* **Weights and Bias**: Initialize the weight vector  (matching the number of features) and a scalar bias , usually to zeros or small random numbers.\n",
    "* **Hyperparameters**: Define the **Learning Rate** $(\\alpha)$ and the number of **Epochs** (iterations) for the training process.\n",
    "## **2. The Forward Pass (Probabilistic Modeling)**\n",
    "For each input feature vector :\n",
    "* **Linear Combination**: Calculate the linear sum: $$ z = w^\\top x + b $$\n",
    "* **Sigmoid Activation**:  \n",
    "  Pass $( z )$ through the **sigmoid function** to squash the output between 0 and 1:  \n",
    "   $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "This output represents the probability $ ( \\hat{y} ) $ that the input belongs to the positive class.\n",
    "\n",
    "## **3. The Loss Function (Cost Calculation)**\n",
    "* **Binary Cross-Entropy**: Measure the error between the predicted probability $ ( \\hat{y} ) $ and the actual label $ ( y ) $ using the Log Loss formula:\n",
    "$$\n",
    "  J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m}\n",
    "  \\left[\n",
    "    y^{(i)} \\log(\\hat{y}^{(i)}) +\n",
    "    (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\n",
    "  \\right]\n",
    "  $$\n",
    "\n",
    "## **4. Optimization (Gradient Descent)**\n",
    "Update the parameters to minimize the loss:\n",
    "* **Compute Gradients**: Calculate the partial derivatives of the loss with respect to  and .\n",
    "* **Update Parameters**: Move the weights in the opposite direction of the gradient:\n",
    "$$\n",
    "w = w - \\alpha \\cdot dw\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha \\cdot db\n",
    "$$\n",
    "\n",
    "## **5. Prediction and Evaluation**\n",
    "* **Thresholding**: Convert the probabilistic output into a binary class. If $ \\hat{y} >= 0.5 $ , predict class 1; otherwise, predict class 0.\n",
    "* **Performance Metrics**: Evaluate the model using accuracy, precision, or a confusion matrix.\n",
    "* **Failure Analysis**: Identify samples where the linear decision boundary fails to separate the classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "07af3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15745b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate: float = 0.01, epochs: int = 1000):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        # Sigmoid function and appropriate loss formulation \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialization \n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Logistic Regression with gradient descent \n",
    "        for _ in range(self.epochs):\n",
    "            # Forward Pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            # Compute Loss\n",
    "            epsilon = 1e-15 # to avoid log(0)\n",
    "            loss = -1/m * np.sum(y * np.log(y_predicted + epsilon) + (1 - y) * np.log(1 - y_predicted + epsilon))\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            # Compute Gradients\n",
    "            dw = (1 / m) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / m) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            # Print loss every 100 epochs\n",
    "            if _ % 100 == 0:\n",
    "                print(f\"Epoch {_}: Loss {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        # Threshold at 0.5 for binary classification\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "b257517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv('./data/data.csv') \n",
    "\n",
    "# 1. Drop any completely empty columns (trailing commas)\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# 2. Drop the ID column (usually the first column)\n",
    "df = df.drop(df.columns[0], axis=1) \n",
    "\n",
    "# 3. Encode Labels: Convert 'M' to 1 and 'B' to 0\n",
    "# The Diagnosis column is now at index 0 after dropping ID\n",
    "diagnosis_col = df.columns[0]\n",
    "df[diagnosis_col] = df[diagnosis_col].map({'M': 1, 'B': 0})\n",
    "\n",
    "# 4. Separate Target (y) and Features (X)\n",
    "# y is the Diagnosis (0 or 1)\n",
    "# X is all the numeric features that follow\n",
    "y = df.iloc[:, 0].values\n",
    "X = df.iloc[:, 1:].values\n",
    "\n",
    "# 5. Manual Scaling (Only now that everything is numeric)\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "std[std == 0] = 1.0 # Prevent division by zero\n",
    "\n",
    "X = (X - mean) / (std + 1e-15)\n",
    "\n",
    "# 6. Dataset 80/20 split\n",
    "split_ratio = 0.8\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "split_idx = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[indices[:split_idx]], X[indices[split_idx:]]\n",
    "y_train, y_test = y[indices[:split_idx]], y[indices[split_idx:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "5868d099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.6931471805599435\n",
      "Epoch 100: Loss 0.10459886821112035\n",
      "Epoch 200: Loss 0.08676700297611707\n",
      "Epoch 300: Loss 0.07895507819699642\n",
      "Epoch 400: Loss 0.07427325229837614\n",
      "Epoch 500: Loss 0.07104204784987246\n",
      "Epoch 600: Loss 0.06862635412961934\n",
      "Epoch 700: Loss 0.06672550452421155\n",
      "Epoch 800: Loss 0.06517569007561824\n",
      "Epoch 900: Loss 0.0638786144438558\n",
      "Epoch 1000: Loss 0.0627709987922777\n",
      "Epoch 1100: Loss 0.061809882184548096\n",
      "Epoch 1200: Loss 0.06096486363582449\n",
      "Epoch 1300: Loss 0.06021370663901429\n",
      "Epoch 1400: Loss 0.059539702518481\n",
      "Epoch 1500: Loss 0.05893001294386727\n",
      "Epoch 1600: Loss 0.05837458643136835\n",
      "Epoch 1700: Loss 0.05786542626902604\n",
      "Epoch 1800: Loss 0.05739608173440556\n",
      "Epoch 1900: Loss 0.05696128580364125\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = LogisticRegression(learning_rate=0.1, epochs=2000)\n",
    "\n",
    "# Train using gradient descent logic\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and convert to classes\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1fe662b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Evaluation ---\n",
      "Accuracy:  0.9825\n",
      "Precision: 0.9773\n",
      "Recall:    0.9773\n"
     ]
    }
   ],
   "source": [
    "# Convert lists/arrays to integer numpy arrays for reliable comparison\n",
    "y_true = np.array(y_test).astype(int)\n",
    "y_p = np.array(y_pred).astype(int)\n",
    "\n",
    "# 1. Component calculation\n",
    "tp = np.sum((y_true == 1) & (y_p == 1)) # True Malignant correctly identified\n",
    "tn = np.sum((y_true == 0) & (y_p == 0)) # True Benign correctly identified\n",
    "fp = np.sum((y_true == 0) & (y_p == 1)) # Benign mistaken for Malignant\n",
    "fn = np.sum((y_true == 1) & (y_p == 0)) # Malignant missed\n",
    "\n",
    "# 2. Metric calculation\n",
    "accuracy = (tp + tn) / len(y_true)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"--- Final Evaluation ---\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic-machine-learning-YT3iF3lf-py3.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
